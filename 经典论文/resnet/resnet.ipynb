{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81830f7a",
   "metadata": {},
   "source": [
    "# 第一遍\n",
    "## 摘要\n",
    "- 深的神经网络难以训练\n",
    "- 使用残差学习,使训练容易很多\n",
    "- 152层,<font color=\"red\">8倍于vgg但复杂度更低</font>\n",
    "## 结论(无)\n",
    "## 关键图表\n",
    "\n",
    "- <img src=\"屏幕截图 2025-07-03 215047.png\" width=\"300px\"><br>\n",
    "- 正常情况下,层数过高并不代表模型效果就更好(不是过拟合,在训练和验证上表现都不好)<br>\n",
    "<img src=\"屏幕截图 2025-07-03 215552.png\" width=\"300px\"><br>\n",
    "- 在加入残差连接后,模型表现变优"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0b9c5c",
   "metadata": {},
   "source": [
    "# 第二遍\n",
    "## introduction\n",
    "- 堆叠更多层模型就更好了吗?->不是,层数堆叠导致训练误差变高(不是过拟合)\n",
    "\n",
    "- 下层和比较浅的网络一致,上层变为恒等映射(identity mapping)\n",
    "\n",
    "### deep residual learning network\n",
    "- 学习的内容:<br>\n",
    "- 假设我想学的是$\\mathcal{H} (x)$\n",
    "- 1.:一部分层学习内容和比较浅的网络一致,试图输出一个x\n",
    "- 2.:另一部分学习的内容是输出值和真实值的残差,记为$\\mathcal{F} (x)=\\mathcal{H} (x)-x$<br>\n",
    "<img src=\"屏幕截图 2025-07-03 221549.png\" width=\"300px\"><br>\n",
    "- 方法:residual connection\n",
    "## related work\n",
    "### 输入输出形状不同\n",
    "- 添加额外的0\n",
    "- <font color=\"red\">1 * 1卷积调整通道数</font>\n",
    "- 没有全链接层->没有dropout\n",
    "- <font color=\"red\">bottle neck设计</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab146b30",
   "metadata": {},
   "source": [
    "## 实验\n",
    "- map精度\n",
    "- cnn的主干模型换成resnet\n",
    "- resnet如何避免梯度消失:<br>\n",
    "\n",
    "<img src=\"屏幕截图 2025-07-04 124411.png\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37fce95",
   "metadata": {},
   "source": [
    "# 总结\n",
    "- 解决的问题:神经网络堆叠层数模型性能未见显著提升,分别训练残差和预想输出"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
