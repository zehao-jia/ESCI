# SSM的起源
## 1.1 Transformer的二次复杂度
- 现有大模型处理长文本算力消耗巨大,原因是Transformer架构中注意力机制的二次复杂度
- *1.一方面,出现针对注意力机制的魔改,也有s4,flashattention等*
- *二方面,s4,flashattention的作者也提出了新的序列模型--mamba*
## 1.2 RNN:隐藏状态$h_t$基于当前输入和前一个隐藏状态$h_{t-1}$
![alt text](<屏幕截图 2025-08-15 153114.png>)
- 问题1:$h_t$只包含此前若干步而非所有步
- 问题2:无法并行训练
## 1.3 状态空间与SSM
### 1.3.1 什么是状态空间
- 以迷宫为例
![alt text](<屏幕截图 2025-08-15 153703.png>)
- 上述迷宫可以简单建模为一个'状态空间表示',其中的每一个小框显示
> 1.当前你的x坐标<br>
> 2.当前的y坐标<br>
> 3.到出口距离<br>
- 描述这些的向量可以表示为'状态向量'
### 1.3.2状态空间模型state space model(SSM)
- SSM是用于描述这些状态并根据某些输入预测下一个状态可能是什么的模型
- 构成:
> 映射输入序列x(t),比如迷宫中的向左和向下运动<br>
> 到潜在状态表示h(t),比如到出口的距离<br>
> 并导出预测输出序列y(t),比如再次向左移动以更快到达出口<br>
![alt text](<屏幕截图 2025-08-15 155022.png>)
